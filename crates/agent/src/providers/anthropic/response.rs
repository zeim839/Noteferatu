use serde::{Serialize, Deserialize};
use super::message::*;
use crate::core::{self, Role};

/// Response from the [completion](crate::core::Client::completion)
/// route.
///
/// Contains the LLMs response, including tool use, web search, and
/// textual results.
#[derive(Default, Debug, Clone, Serialize, Deserialize)]
pub struct Response {

    /// Unique object identifier.
    pub id: String,

    /// Conversational role of the generated message.
    pub role: Role,

    /// Content generated by the model.
    ///
    /// This is an array of content blocks, each of which has a type
    /// that determines its shape.
    pub content: Vec<Content>,

    /// The model that handled the request.
    pub model: String,

    /// The reason the response stopped.
    pub stop_reason: Option<StopReason>,

    /// Billing and rate-limit usage.
    pub usage: Usage,
}

impl Into<core::Response> for Response {
    fn into(self) -> core::Response {
        let role = self.role;
        let messages = self.content.into_iter().filter_map(|c| {
            let content = match c.kind {
                ContentKind::Text => Some(core::MessageContent::Text(c.text.unwrap_or_default())),
                ContentKind::Thinking => Some(core::MessageContent::Text(format!(
                    "<thinking>{}</thinking>",
                    c.text.unwrap_or_default()
                ))),
                ContentKind::ToolUse => Some(core::MessageContent::ToolCall(core::ToolCall {
                    id: c.tool_use_id.unwrap_or_default(),
                    name: c.name.unwrap_or_default(),
                    arguments: c.input.unwrap_or(serde_json::Value::Null),
                })),
                _ => None,
            };

            content.map(|content| core::Message {
                role: role.clone(),
                content,
            })
        }).collect();

        let usage = core::Usage {
            prompt_tokens: self.usage.input_tokens.unwrap_or(0),
            completion_tokens: self.usage.output_tokens.unwrap_or(0),
            total_tokens: self.usage.input_tokens.unwrap_or(0)
                + self.usage.output_tokens.unwrap_or(0),
        };

        core::Response { messages, usage }
    }
}

/// Reports on the reason the LLM stopped generating tokens.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum StopReason {

    /// The model reached a natural stopping point.
    EndTurn,

    /// Exceeded the requested max_tokens or the model's maximum.
    MaxTokens,

    /// One of the custom `stop_sequences` was generated.
    StopSequence,

    /// The model invoked one or more tools.
    ToolUse,

    /// Paused a long-running turn.
    PauseTurn,

    /// Potential policy violation.
    Refusal,
}

/// Billing and rate-limit usage.
#[derive(Default, Debug, Clone, Serialize, Deserialize)]
pub struct Usage {

    /// The number of input tokens which were used.
    pub input_tokens: Option<i64>,

    /// The number of output tokens which were used.
    pub output_tokens: Option<i64>,
}
