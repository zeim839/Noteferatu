use serde::{Serialize, Deserialize};
use super::error::GoogleError;
use super::content::*;
use crate::core;

/// Response from the [Model](super::model::Model) supporting
/// multiple candidate responses.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct Response {

    /// Candidate responses from the model.
    #[serde(default)]
    pub candidates: Vec<Candidate>,

    /// Metadata on the generation requests' token usage.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub usage_metadata: Option<UsageMetadata>,

    /// The model version used to generate the response.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub model_version: Option<String>,

    /// Used to identify each response.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub response_id: Option<String>,

    /// Responses from streaming possibly include errors.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub error: Option<GoogleError>,
}

impl From<super::content::Role> for core::Role {
    fn from(value: super::content::Role) -> Self {
        match value {
            super::content::Role::Model => core::Role::Assistant,
            super::content::Role::User => core::Role::User,
            super::content::Role::Function => core::Role::Tool,
        }
    }
}

impl Into<core::Response> for Response {
    fn into(self) -> core::Response {
        let mut messages = Vec::new();
        for candidate in self.candidates {
            let role: core::Role = candidate.content.role.into();

            for part in candidate.content.parts {
                if let Some(text) = part.data.text {
                    if !text.is_empty() {
                        messages.push(core::Message {
                            role: role.clone(),
                            content: core::MessageContent::Text(text),
                        });
                    }
                }
                if let Some(function_call) = part.data.function_call {
                    messages.push(core::Message {
                        role: role.clone(),
                        content: core::MessageContent::ToolCall(core::ToolCall {
                            id: function_call.id.unwrap_or_default(),
                            name: function_call.name,
                            arguments: function_call.args.unwrap_or(serde_json::Value::Null),
                        }),
                    });
                }
            }
        }

        let usage = if let Some(usage_metadata) = self.usage_metadata {
            core::Usage {
                prompt_tokens: usage_metadata.prompt_token_count.unwrap_or(0),
                completion_tokens: usage_metadata.candidates_token_count.unwrap_or(0),
                total_tokens: usage_metadata.total_token_count.unwrap_or(0),
            }
        } else {
            core::Usage::default()
        };

        let error = self.error.map(|err| core::Error::Google(err));
        core::Response { messages, usage, error }
    }
}

/// A response candidate generated from the model.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct Candidate {

    /// Generated content returned from the model.
    pub content: Content,

    /// The reason why the model stopped generating tokens.
    /// If empty, the model has not stopped generating tokens.
    pub finish_reason: Option<FinishReason>,
}

/// Defines the reason why the model stopped generating tokens.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "SCREAMING_SNAKE_CASE")]
pub enum FinishReason {

    /// This value is unused.
    FinishReasonUnspecified,

    /// Natural stop point of the model or provided stop sequence.
    Stop,

    /// The maximum number of tokens as specified in the request was
    /// reached.
    MaxTokens,

    /// The response candidate content was flagged for safety reasons.
    Safety,

    /// The response candidate content was flagged for recitation
    /// reasons.
    Recitation,

    /// The response candidate content was flagged for using an
    /// unsupported language.
    Language,

    /// Unknown reason.
    Other,

    /// Token generation stopped because the content contains
    /// forbidden terms.
    Blocklist,

    /// Token generation stopped for potentially containing prohibited
    /// content.
    ProhibitedContent,

    /// Token generation stopped because the content potentially
    /// contains Sensitive Personally Identifiable Information (SPII).
    Spii,

    /// The function call generated by the model is invalid.
    MalformedFunctionCall,

    /// Token generation stopped because generated images contain
    /// safety violations.
    ImageSafety,

    /// Model generated a tool call but no tools were enabled in the
    /// request.
    UnexpectedToolCall,
}

/// Metadata on the generation request's token usage.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct UsageMetadata {

    /// Number of tokens in the prompt.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub prompt_token_count: Option<i64>,

    /// Number of tokens in the cached part of the prompt (the cached
    /// content)
    pub cached_content_token_count: Option<i64>,

    /// Total number of tokens across all the generated response
    /// candidates.
    pub candidates_token_count: Option<i64>,

    /// Number of tokens present in tool-use prompt(s).
    pub tool_use_prompt_token_count: Option<i64>,

    /// Number of tokens of thoughts for thinking models.
    pub thoughts_token_count: Option<i64>,

    /// Total token count for the generation request (prompt +
    /// response candidates)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub total_token_count: Option<i64>,
}
