use serde::{Serialize, Deserialize};

/// Chat message role.
#[derive(Debug, Serialize, Deserialize)]
pub enum Role {

    /// Developer-provided instructions that the model should follow,
    /// regardless of messages sent by the user. With o1 models and
    /// newer, `developer` messages replace the previous system
    /// messages.
    #[serde(rename = "developer")]
    Developer,

    /// Developer-provided instructions that the model should follow,
    /// regardless of messages sent by the user. With o1 models and
    /// newer, use `developer` messages for this purpose instead.
    #[serde(rename = "system")]
    System,

    /// Messages sent by an end user, containing prompts or additional
    /// context information.
    #[serde(rename = "user")]
    User,

    /// Messages sent by the model in response to user messages.
    #[serde(rename = "assistant")]
    Assistant,

    /// Tool call messages.
    #[serde(rename = "tool")]
    Tool,

}

#[derive(Debug, Serialize, Deserialize)]
pub struct ToolCall {

    /// The ID of the tool call.
    pub id: String,

    /// The type of the tool call.
    ///
    /// Currently, only `function` is supported.
    #[serde(rename = "type")]
    pub kind: String,

    /// The function that the model called.
    pub function: FunctionCall,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct FunctionCall {

    /// The arguments to call the function with.
    ///
    /// Note that the model does not always generate valid JSON, and
    /// may hallucinate parameters not defined by your function
    /// schema. Validate the arguments in your code before calling
    /// your function.
    pub arguments: String,

    /// The name of the function to call.
    pub name: String,
}

/// Chat message.
#[derive(Debug, Serialize, Deserialize)]
pub struct Message {

    /// The role of the messages author.
    pub role: Option<Role>,

    /// The contents of the message.
    pub content: Option<String>,

    /// Tool call that this message is responding to.
    ///
    /// Specify only when `role` is `Tool`.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_call_id: Option<String>,

    /// The refusal message by the assistant.
    ///
    /// Only specified when `assistant` responses refuse a prompt.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub refusal: Option<String>,

    /// The tool calls generated by the model, such as function calls.
    ///
    /// Only specified in `assistant` responses that use tool calls.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_calls: Option<Vec<ToolCall>>,

}

/// ChatRequest represents a chat completion request.
///
/// API Reference: [Create Chat Completion](https://platform.openai.com/docs/api-reference/chat/create)
#[derive(Default, Serialize, Deserialize)]
pub struct ChatRequest {

    /// A list of messages comprising the conversation so
    /// far. Depending on the model you use, different message types
    /// (modalities) are supported, like text, images, and audio.
    pub messages: Vec<Message>,

    /// The model ID to use.
    pub model: String,

    /// Number between -2.0 and 2.0. Positive values penalize new
    /// tokens based on their existing frequency in the text so far,
    /// decreasing the model's likelihood to repeat the same line
    /// verbatim.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub frequency_penalty: Option<i64>,

    /// An upper bound for the number of tokens that can be generated
    /// for a completion, including visible output tokens and
    /// reasoning tokens.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_completion_tokens: Option<i64>,

    /// How many chat completion choices to generate for each input
    /// message. Note that you will be charged based on the number of
    /// generated tokens across all of the choices. Keep `n` as 1 to
    /// minimize costs.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub n: Option<i64>,

    /// Whether to enable parallel function calling during tool use.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub parallel_tool_calls: Option<bool>,

    /// Number between -2.0 and 2.0. Positive values penalize new
    /// tokens based on whether they appear in the text so far,
    /// increasing the model's likelihood to talk about new topics.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub presence_penalty: Option<i64>,

    /// Constrains effort on reasoning for reasoning models. Currently
    /// supported values are `low`, `medium`, and `high`. Reducing
    /// reasoning effort can result in faster responses and fewer
    /// tokens used on reasoning in a response.
    ///
    /// o-series models only.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub reasoning_effort: Option<ReasoningEffort>,

    /// Specifies the processing type used for serving the request.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub service_tier: Option<ServiceTier>,

    /// If set to true, the model response data will be streamed to
    /// the client as it is generated using server-sent events.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,

    /// Options for streaming response.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream_options: Option<StreamOptions>,

    /// What sampling temperature to use, between 0 and 2. Higher
    /// values like 0.8 will make the output more random, while lower
    /// values like 0.2 will make it more focused and
    /// deterministic. We generally recommend altering this or top_p
    /// but not both.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<i64>,

    /// A list of tools the model may call. Currently, only functions
    /// are supported as a tool. Use this to provide a list of
    /// functions the model may generate JSON inputs for. A max of 128
    /// functions are supported.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tools: Option<Vec<ToolDefinition>>,

    /// An alternative to sampling with temperature, called nucleus
    /// sampling, where the model considers the results of the tokens
    /// with top_p probability mass. So 0.1 means only the tokens
    /// comprising the top 10% probability mass are considered.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<i64>,

    /// This tool searches the web for relevant results to use in a
    /// response.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub web_search_options: Option<WebSearchOptions>,
}

impl ChatRequest {

    /// Creates a [ChatRequest] from a single text prompt.
    pub fn from_prompt(model: &str, prompt: &str) -> Self {
        let mut req = Self::default();
        req.model = model.to_string();
        req.messages = vec![Message {
            role: Some(Role::User),
            content: Some(prompt.to_string()),
            tool_call_id: None,
            refusal: None,
            tool_calls: None,
        }];
        req
    }

    /// Creates a [ChatRequest] from a vector of messages.
    pub fn from_messages(model: &str, messages: Vec<Message>) -> Self {
        let mut req = Self::default();
        req.model = model.to_string();
        req.messages = messages;
        req
    }

    /// Populates the [Self::frequency_penalty] field with the given value.
    pub fn with_frequency_penaly(self, frequency_penalty: Option<i64>) -> Self {
        Self { frequency_penalty, ..self }
    }

    /// Populates the [Self::max_completion_tokens] field with the given value.
    pub fn with_max_completion_tokens(self, max_completion_tokens: Option<i64>) -> Self {
        Self { max_completion_tokens, ..self }
    }

    /// Populates the [Self::n] field with the given value.
    pub fn with_n(self, n: Option<i64>) -> Self {
        Self { n, ..self }
    }

    /// Populates the [Self::parallel_tool_calls] field with the given value.
    pub fn with_parallel_tool_calls(self, parallel_tool_calls: Option<bool>) -> Self {
        Self { parallel_tool_calls, ..self }
    }

    /// Populates the [Self::presence_penalty] field with the given value.
    pub fn with_presence_penalty(self, presence_penalty: Option<i64>) -> Self {
        Self { presence_penalty, ..self }
    }

    /// Populates the [Self::reasoning_effort] field with the given value.
    pub fn with_reasoning_effort(self, reasoning_effort: Option<ReasoningEffort>) -> Self {
        Self { reasoning_effort, ..self }
    }

    /// Populates the [Self::service_tier] field with the given value.
    pub fn with_service_tier(self, service_tier: Option<ServiceTier>) -> Self {
        Self { service_tier, ..self }
    }

    /// Populates the [Self::stream] field with the given value.
    pub fn with_stream(self, stream: Option<bool>) -> Self {
        Self { stream, ..self }
    }

    /// Populates the [Self::stream_options] field with the given value.
    pub fn with_stream_options(self, stream_options: Option<StreamOptions>) -> Self {
        Self { stream_options, ..self }
    }

    /// Populates the [Self::temperature] field with the given value.
    pub fn with_temperature(self, temperature: Option<i64>) -> Self {
        Self { temperature, ..self }
    }

    /// Populates the [Self::tools] field with the given value.
    pub fn with_tools(self, tools: Option<Vec<ToolDefinition>>) -> Self {
        Self { tools, ..self }
    }

    /// Populates the [Self::top_p] field with the given value.
    pub fn with_top_p(self, top_p: Option<i64>) -> Self {
        Self { top_p, ..self }
    }

    /// Populates the [Self::web_search_options] field with the given value.
    pub fn with_web_search_options(self, web_search_options: Option<WebSearchOptions>) -> Self {
        Self { web_search_options, ..self }
    }
}

#[derive(Serialize, Deserialize)]
pub enum ReasoningEffort {
    #[serde(rename = "low")]
    Low,
    #[serde(rename = "medium")]
    Medium,
    #[serde(rename = "high")]
    High,
}

/// Specifies the processing type used for serving the request.
#[derive(Debug, Serialize, Deserialize)]
pub enum ServiceTier {
    /// The request will be processed with the service tier configured
    /// in the Project settings. Unless otherwise configured, the
    /// Project will use 'default'.
    #[serde(rename = "auto")]
    Auto,

    /// The request will be processed with the standard pricing and
    /// performance for the selected model.
    #[serde(rename = "default")]
    Default,

    /// The request will be processed with the 'flex' service tier.
    #[serde(rename = "flex")]
    Flex,

    /// The request will be processed with the 'priority' service tier.
    #[serde(rename = "priority")]
    Priority,
}

/// Options for streaming response.
#[derive(Serialize, Deserialize)]
pub struct StreamOptions {

    /// If set, an additional chunk will be streamed before the data:
    /// `[DONE]` message. The usage field on this chunk shows the token
    /// usage statistics for the entire request, and the choices field
    /// will always be an empty array.
    ///
    /// All other chunks will also include a usage field, but with a
    /// null value. NOTE: If the stream is interrupted, you may not
    /// receive the final usage chunk which contains the total token
    /// usage for the request.
    pub include_usage: Option<bool>,
}

/// Tool Definition.
#[derive(Serialize, Deserialize)]
pub struct ToolDefinition {

    /// The type of the tool. Currently, only `function` is supported.
    #[serde(rename = "type")]
    pub kind: String,

    /// Tool function definition.
    pub function: FunctionDefinition,
}

/// Function Definition.
#[derive(Serialize, Deserialize)]
pub struct FunctionDefinition {

    /// The name of the function to be called. Must be a-z, A-Z, 0-9,
    /// or contain underscores and dashes, with a maximum length of
    /// 64.
    pub name: String,

    /// A description of what the function does, used by the model to
    /// choose when and how to call the function.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub description: Option<String>,

    /// The parameters the functions accepts.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub parameters: Option<serde_json::Value>,

    /// Whether to enable strict schema adherence when generating the
    /// function call. If set to true, the model will follow the exact
    /// schema defined in the `parameters` field.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub strict: Option<bool>,
}

/// Web search options: this tool searches the web for relevant
/// results to use in a response.
#[derive(Serialize, Deserialize)]
pub struct WebSearchOptions {

    /// High level guidance for the amount of context window space to
    /// use for the search. One of `low`, `medium`, or `high`. medium
    /// is the default.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub search_context_size: Option<SearchContextSize>,
}

/// High level guidance for the amount of context window space to
/// use for the search.
#[derive(Serialize, Deserialize)]
pub enum SearchContextSize {
    #[serde(rename = "low")]
    Low,
    #[serde(rename = "medium")]
    Medium,
    #[serde(rename = "high")]
    High,
}

/// ChatResponse represents a successful chat completion.
#[derive(Debug, Serialize, Deserialize)]
pub struct ChatResponse {

    /// A list of chat completion choices. Can be more than one if
    /// [ChatRequest::n] is greater than 1.
    pub choices: Vec<Choice>,

    /// The Unix timestamp (in seconds) of when the chat completion
    /// was created.
    pub created: i64,

    /// A unique identifier for the chat completion.
    pub id: String,

    /// The model used for the chat completion.
    pub model: String,

    /// Specifies the processing type used for serving the request.
    pub service_tier: Option<ServiceTier>,

    /// Usage statistics for the completion request.
    pub usage: Option<Usage>,

}

#[derive(Debug, Serialize, Deserialize)]
pub struct Choice {

    /// The reason the model stopped generating tokens.
    pub finish_reason: Option<FinishReason>,

    /// The index of the choice in the list of choices.
    pub index: i64,

    /// The index of the choice in the list of choices.
    pub message: Option<Message>,

    /// Change since the last choice output (used for streaming
    /// completions).
    pub delta: Option<Message>,
}

/// The reason the model stopped generating tokens.
#[derive(Debug, Serialize, Deserialize)]
pub enum FinishReason {

    /// `stop` if the model hit a natural stop point or a provided
    /// stop sequence.
    #[serde(rename = "stop")]
    Stop,

    /// `length` if the maximum number of tokens specified in the
    /// request was reached.
    #[serde(rename = "length")]
    Length,

    /// `content_filter` if content was omitted due to a flag from our
    /// content filters
    #[serde(rename = "content_filter")]
    ContentFilter,

    /// `tool_calls` if the model called a tool.
    #[serde(rename = "tool_calls")]
    ToolCalls,

    /// `function_call` (deprecated) if the model called a function.
    #[serde(rename = "function_call")]
    FunctionCall,
}

/// Usage statistics for the completion request.
#[derive(Debug, Serialize, Deserialize)]
pub struct Usage {

    /// Number of tokens in the generated completion.
    pub completion_tokens: i64,

    /// Number of tokens in the prompt.
    pub prompt_tokens: i64,

    /// Total number of tokens used in the request (prompt +
    /// completion).
    pub total_tokens: i64,
}
