use serde::{Serialize, Deserialize};

/// Chat message role.
#[derive(Debug, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum Role {

    /// Developer-provided instructions that the model should follow,
    /// regardless of messages sent by the user. With o1 models and
    /// newer, `developer` messages replace the previous system
    /// messages.
    Developer,

    /// Developer-provided instructions that the model should follow,
    /// regardless of messages sent by the user. With o1 models and
    /// newer, use `developer` messages for this purpose instead.
    System,

    /// Messages sent by an end user.
    User,

    /// Messages sent by the model in response to user messages.
    Assistant,

    /// Tool call messages.
    Tool,
}

/// Describes an instance of an LLM tool call.
#[derive(Debug, Serialize, Deserialize)]
pub struct ToolCall {

    /// The ID of the tool call.
    pub id: Option<String>,

    /// The type of the tool call.
    ///
    /// Currently, only `function` is supported.
    #[serde(rename = "type")]
    pub kind: Option<String>,

    /// The function that the model called.
    pub function: FunctionCall,
}

/// Describes the function called by a an associated [ToolCall].
#[derive(Debug, Serialize, Deserialize)]
pub struct FunctionCall {

    /// The arguments to call the function with.
    ///
    /// Note that the model does not always generate valid JSON, and
    /// may hallucinate parameters not defined by your function
    /// schema. Validate the arguments in your code before calling
    /// your function.
    pub arguments: serde_json::Value,

    /// The name of the function to call.
    pub name: String,
}

/// Chat message.
#[derive(Debug, Serialize, Deserialize)]
pub struct Message {

    /// The role of the messages author.
    pub role: Option<Role>,

    /// The contents of the message.
    pub content: Option<Content>,

    /// Tool call that this message is responding to.
    ///
    /// Specify only when `role` is `Tool`.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_call_id: Option<String>,

    /// The refusal message by the assistant.
    ///
    /// Only specified when `assistant` responses refuse a prompt.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub refusal: Option<String>,

    /// The tool calls generated by the model, such as function calls.
    ///
    /// Only specified in `assistant` responses that use tool calls.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_calls: Option<Vec<ToolCall>>,
}

/// Describes the possible response types of a [Message].
#[derive(Debug, Serialize, Deserialize)]
#[serde(untagged)]
pub enum Content {
    Text(String),
    Content(Vec<ContentData>),
}

#[derive(Debug, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ContentKind {
    InputText,
    InputFile,
    Text,
    ImageUrl,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ContentData {

    #[serde(rename = "type")]
    pub kind: String,

    /// A basic text response.
    pub text: Option<String>,

    /// URL to an image to be embedded as context.
    pub image_url: Option<String>,

    /// URL to an file/document to be embedded as context.
    pub file_url: Option<String>,

    /// Name of the file whose data is being uploaded. Associated file
    /// data is sent as a base64 in the [Self::file_data] field.
    pub filename: Option<String>,

    /// Base64 encoded file data to add as context.
    pub file_data: Option<String>,
}

/// ChatRequest represents a chat completion request.
///
/// API Reference: [Create Chat Completion](https://platform.openai.com/docs/api-reference/chat/create)
#[derive(Default, Serialize, Deserialize)]
pub struct ChatRequest {

    /// A list of messages comprising the conversation so
    /// far. Depending on the model you use, different message types
    /// (modalities) are supported, like text, images, and audio.
    pub messages: Vec<Message>,

    /// The model ID to use.
    pub model: String,

    /// An upper bound for the number of tokens that can be generated
    /// for a completion, including visible output tokens and
    /// reasoning tokens.
    #[serde(skip_serializing_if = "Option::is_none", alias = "max_tokens")]
    pub max_completion_tokens: Option<i64>,

    /// If set to true, the model response data will be streamed to
    /// the client as it is generated using server-sent events.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,

    /// A list of tools the model may call. Currently, only functions
    /// are supported as a tool. Use this to provide a list of
    /// functions the model may generate JSON inputs for. A max of 128
    /// functions are supported.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tools: Option<Vec<ToolDefinition>>,

    /// Whether a model's response may include multiple tool calls.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub parallel_tool_calls: Option<bool>,
}

impl ChatRequest {

    /// Creates a [ChatRequest] from a single text prompt.
    pub fn from_prompt(model: &str, prompt: &str) -> Self {
        let mut req = Self::default();
        req.model = model.to_string();
        req.messages = vec![Message {
            role: Some(Role::User),
            content: Some(Content::Text(prompt.to_string())),
            tool_call_id: None,
            refusal: None,
            tool_calls: None,
        }];
        req
    }

    /// Creates a [ChatRequest] from a vector of messages.
    pub fn from_messages(model: &str, messages: Vec<Message>) -> Self {
        let mut req = Self::default();
        req.model = model.to_string();
        req.messages = messages;
        req
    }

    /// Populates the [Self::max_completion_tokens] field with the given value.
    pub fn with_max_completion_tokens(self, max_completion_tokens: Option<i64>) -> Self {
        Self { max_completion_tokens, ..self }
    }

    /// Populates the [Self::parallel_tool_calls] field with the given value.
    pub fn with_parallel_tool_calls(self, parallel_tool_calls: Option<bool>) -> Self {
        Self { parallel_tool_calls, ..self }
    }

    /// Populates the [Self::stream] field with the given value.
    pub fn with_stream(self, stream: Option<bool>) -> Self {
        Self { stream, ..self }
    }

    /// Populates the [Self::tools] field with the given value.
    pub fn with_tools(self, tools: Option<Vec<ToolDefinition>>) -> Self {
        Self { tools, ..self }
    }
}

/// Tool Definition.
#[derive(Serialize, Deserialize)]
pub struct ToolDefinition {

    /// The type of the tool. Currently, only `function` is supported.
    #[serde(rename = "type")]
    pub kind: String,

    /// Tool function definition.
    pub function: FunctionDefinition,
}

/// Function Definition.
#[derive(Clone, Serialize, Deserialize)]
pub struct FunctionDefinition {

    /// The name of the function to be called. Must be a-z, A-Z, 0-9,
    /// or contain underscores and dashes, with a maximum length of
    /// 64.
    pub name: String,

    /// A description of what the function does, used by the model to
    /// choose when and how to call the function.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub description: Option<String>,

    /// The parameters the functions accepts.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub parameters: Option<serde_json::Value>,

    /// Whether to enable strict schema adherence when generating the
    /// function call. If set to true, the model will follow the exact
    /// schema defined in the `parameters` field.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub strict: Option<bool>,
}

/// ChatResponse represents a successful chat completion.
#[derive(Debug, Serialize, Deserialize)]
pub struct ChatResponse {

    /// A list of chat completion choices. Can be more than one if
    /// [ChatRequest::n] is greater than 1.
    pub choices: Vec<Choice>,

    /// The Unix timestamp (in seconds) of when the chat completion
    /// was created.
    pub created: i64,

    /// A unique identifier for the chat completion.
    pub id: String,

    /// The model used for the chat completion.
    pub model: String,

    /// Usage statistics for the completion request.
    pub usage: Option<Usage>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Choice {

    /// The reason the model stopped generating tokens.
    pub finish_reason: Option<FinishReason>,

    /// The index of the choice in the list of choices.
    pub index: i64,

    /// The index of the choice in the list of choices.
    pub message: Option<Message>,

    /// Change since the last choice output (used for streaming
    /// completions).
    pub delta: Option<Message>,
}

/// The reason the model stopped generating tokens.
#[derive(Debug, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum FinishReason {

    /// `stop` if the model hit a natural stop point or a provided
    /// stop sequence.
    Stop,

    /// `length` if the maximum number of tokens specified in the
    /// request was reached.
    Length,

    /// `content_filter` if content was omitted due to a flag from our
    /// content filters
    ContentFilter,

    /// `tool_calls` if the model called a tool.
    ToolCalls,

    /// `function_call` (deprecated) if the model called a function.
    FunctionCall,
}

/// Usage statistics for the completion request.
#[derive(Debug, Serialize, Deserialize)]
pub struct Usage {

    /// Number of tokens in the generated completion.
    pub completion_tokens: i64,

    /// Number of tokens in the prompt.
    pub prompt_tokens: i64,

    /// Total number of tokens used in the request (prompt +
    /// completion).
    pub total_tokens: i64,
}
