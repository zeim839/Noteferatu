use serde::{Serialize, Deserialize};
use crate::openai::{ToolCall, ToolDefinition, FunctionDefinition};
use crate::openai::Role;

/// ChatRequest represents a chat completion request.
#[derive(Default, Serialize, Deserialize)]
pub struct ChatRequest {

    /// The model ID to use.
    pub model: String,

    /// A list of messages comprising the conversation so far.
    #[serde(skip_serializing_if = "Vec::is_empty")]
    pub messages: Vec<Message>,

    /// A list of tools that the model may call.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tools: Option<Vec<ToolDefinition>>,

    /// Enable thinking mode (available only for thinking models).
    #[serde(skip_serializing_if = "Option::is_none")]
    pub think: Option<bool>,

    /// Keep model alive for the specified duration.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub keep_alive: Option<f64>,

    /// Whether to stream the response.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,
}

impl ChatRequest {

    /// Creates a [ChatRequest] from a single text prompt.
    pub fn from_prompt(model: &str, prompt: &str) -> Self {
        let mut req = Self::default();
        req.model = model.to_string();
        req.messages = vec![Message {
            role: Role::User,
            content: Some(prompt.to_string()),
            tool_calls: None,
            images: None,
            thinking: None,
        }];
        req
    }

    /// Creates a [ChatRequest] from a vector of messages.
    pub fn from_messages(model: &str, messages: Vec<Message>) -> Self {
        let mut req = Self::default();
        req.model = model.to_string();
        req.messages = messages;
        req
    }

    /// Populates the [Self::tools] field with the given value.
    pub fn with_tools(self, tools: Option<Vec<ToolDefinition>>) -> Self {
        Self { tools, ..self }
    }

    /// Populates the [Self::think] field with the given value.
    pub fn with_think(self, think: Option<bool>) -> Self {
        Self { think, ..self }
    }

    /// Populates the [Self::keep_alive] field with the given value.
    pub fn with_keep_alive(self, keep_alive: Option<f64>) -> Self {
        Self { keep_alive, ..self }
    }

    /// Populates the [Self::stream] field with the given value.
    pub fn with_stream(self, stream: Option<bool>) -> Self {
        Self { stream, ..self }
    }
}

impl crate::Request for ChatRequest {
    fn with_max_tokens(self, _: Option<i64>) -> Self {
        self // not available.
    }

    fn with_temperature(self, _: Option<f64>) -> Self {
        self // not available.
    }

    fn with_web_search_results(self, _: Option<i64>) -> Self {
        self // not available.
    }

    fn with_tools(self, tools: Option<Vec<FunctionDefinition>>) -> Self {
        let tools = tools.map(|v| v.iter().map(|item| {
            ToolDefinition {
                kind: "function".to_string(),
                function: (*item).clone(),
            }
        }).collect());
        Self { tools, ..self }
    }
}

/// Chat message.
#[derive(Debug, Serialize, Deserialize)]
pub struct Message {

    /// The role of the messages author.
    pub role: Role,

    /// The contents of the message.
    pub content: Option<String>,

    /// The tool calls generated by the model, such as function calls.
    ///
    /// Only specified in `assistant` responses that use tool calls.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_calls: Option<Vec<ToolCall>>,

    /// Image inputs.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub images: Option<Vec<String>>,

    /// Thinking text traces (only for thinking models).
    #[serde(skip_serializing_if = "Option::is_none")]
    pub thinking: Option<String>,

}

/// A response to a completion request.
#[derive(Debug, Serialize, Deserialize)]
pub struct ChatResponse {

    /// The model that responded to the request.
    pub model: String,

    /// The response message.
    pub message: Message,

    /// The reason the LLM was terminated.
    pub done_reason: Option<String>,

    /// Whether the model is done responding to the request.
    pub done: bool,

    /// Tokens in the prompt.
    pub prompt_eval_count: Option<i64>,

    /// Total token count.
    pub eval_count: Option<i64>,
}
